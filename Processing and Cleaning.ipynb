{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _________________________________________ \r\n",
      "/  A master was explaining the nature of  \\\r\n",
      "| Tao to one of his novices. \"The Tao is  |\r\n",
      "| embodied in all software -- regardless  |\r\n",
      "| of how insignificant,\" said the master. |\r\n",
      "|                                         |\r\n",
      "| \"Is Tao in a hand-held calculator?\"     |\r\n",
      "| asked the novice.                       |\r\n",
      "|                                         |\r\n",
      "| \"It is,\" came the reply.                |\r\n",
      "|                                         |\r\n",
      "| \"Is the Tao in a video game?\" continued |\r\n",
      "| the novice.                             |\r\n",
      "|                                         |\r\n",
      "| \"It is even in a video game,\" said the  |\r\n",
      "| master.                                 |\r\n",
      "|                                         |\r\n",
      "| \"And is the Tao in the DOS for a        |\r\n",
      "| personal computer?\"                     |\r\n",
      "|                                         |\r\n",
      "| The master coughed and shifted his      |\r\n",
      "| position slightly. \"The lesson is over  |\r\n",
      "| for today,\" he said.                    |\r\n",
      "|                                         |\r\n",
      "| -- Geoffrey James, \"The Tao of          |\r\n",
      "\\ Programming\"                            /\r\n",
      " ----------------------------------------- \r\n",
      "      \\                    / \\  //\\\r\n",
      "       \\    |\\___/|      /   \\//  \\\\\r\n",
      "            /0  0  \\__  /    //  | \\ \\    \r\n",
      "           /     /  \\/_/    //   |  \\  \\  \r\n",
      "           @_^_@'/   \\/_   //    |   \\   \\ \r\n",
      "           //_^_/     \\/_ //     |    \\    \\\r\n",
      "        ( //) |        \\///      |     \\     \\\r\n",
      "      ( / /) _|_ /   )  //       |      \\     _\\\r\n",
      "    ( // /) '/,_ _ _/  ( ; -.    |    _ _\\.-~        .-~~~^-.\r\n",
      "  (( / / )) ,-{        _      `-.|.-~-.           .~         `.\r\n",
      " (( // / ))  '/\\      /                 ~-. _ .-~      .-~^-.  \\\r\n",
      " (( /// ))      `.   {            }                   /      \\  \\\r\n",
      "  (( / ))     .----~-.\\        \\-'                 .~         \\  `. \\^-.\r\n",
      "             ///.----..>        \\             _ -~             `.  ^-`  ^-_\r\n",
      "               ///-._ _ _ _ _ _ _}^ - - - - ~                     ~-- ,.-~\r\n",
      "                                                                  /.-~\r\n"
     ]
    }
   ],
   "source": [
    "'''GENERAL'''\n",
    "import os\n",
    "import collections\n",
    "import glob\n",
    "'''DATA'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import databricks.koalas as ks\n",
    "'''SIGNATURE'''\n",
    "!fortune | cowsay -f dragon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Processing\n",
    "\n",
    "I have a directory of csvs from stat.ink which contains player submitted match data. I want to process them into one cleaned database. Leveraging the power of Koalas, I can use a Spark backend with a Pandas-like interface. I will first clean one csv to devolop a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining csv's\n",
    "My data is currently spread among hundreds of csvs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather list of raw data filenames\n",
    "raw = glob.glob(os.path.join('data/raw', '*.csv'))\n",
    "# Count the number of csvs\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one to inspect\n",
    "test = pd.read_csv(raw[0])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to combine all these into something usable like a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splatoon_concat(path = 'data/raw'):\n",
    "    \"\"\" \n",
    "    Concatanates multiple csvs from one directory into a dataframe\n",
    "  \n",
    "    Parameters: \n",
    "    path (string): optional specified file path\n",
    "  \n",
    "    Returns: \n",
    "    DataFrame: pandas dataframe containing data from all csvs.\n",
    "\n",
    "    \"\"\"\n",
    "    # get filenames\n",
    "    files = glob.glob(os.path.join(path, '*.csv'))\n",
    "    # concat all files\n",
    "    return pd.concat((pd.read_csv(f) for f in files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = splatoon_concat()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Feature Selection\n",
    "\n",
    "Since this data is from match results and I want to create a predictive model for win rate, there are many features that cannot be used as they are only knowable at the end of a match:\n",
    "\n",
    "- period: when the match was played\n",
    "    - irrelevant because this is not a timeseries model\n",
    "    - version number better covers patch changes\n",
    "- time: how long the match took\n",
    "- knockout: if the match was won through the objective as opposed to a time-out\n",
    "- player kill/assist/death/special/inked: player statistics calculated at the end of each match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splatoon_drop(df):\n",
    "    \"\"\" \n",
    "    Drops unnessecary features in stat.ink dataframes\n",
    "  \n",
    "    Parameters: \n",
    "    df (DataFrame): pandas or koalas dataframe on which to drop values\n",
    "  \n",
    "    Returns: \n",
    "    DataFrame: the dataframe with dropped features\n",
    "  \n",
    "    \"\"\"\n",
    "    # initialize list with first features\n",
    "    drop_lst = ['# period','time', 'knockout']\n",
    "    # concatanate player statistics features for each player\n",
    "    for player in ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4']:\n",
    "        drop_lst += [player+'-kill-assist', player+'-kill', player+'-assist',\n",
    "                     player+'-death', player+'-special', player+'-inked']\n",
    "    # drop features in list\n",
    "    return df.drop(drop_lst, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = splatoon_drop(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output as a pickle file for midpoint backup, the next part can get dicey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('data/merge.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load from pickle due to crashing kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.from_pandas(pd.read_pickle('data/merge.pkl'))\n",
    "kdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding the rows\n",
    "\n",
    "Since I am interested in making predictions based off data from one player, I will explode each match data into 8 separate rows, one for every player. The function will drop data from A1 by default as A1 is the player who submitted the data, which can lead to bias. I have more than enough data to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splatoon_explode(df, drop=True):\n",
    "    \"\"\" \n",
    "    Explodes rows of stat.ink dataframes into one row for each player.\n",
    "    This function is intended to be run after splatoon_drop and\n",
    "    is hardcoded to accept its output features.\n",
    "  \n",
    "    Parameters: \n",
    "    df (DataFrame): Pandas dataframe on which to explode\n",
    "  \n",
    "    Returns: \n",
    "    koalas: Exploded dataframe stored in a koalas database\n",
    "  \n",
    "    \"\"\"\n",
    "    # get feature names\n",
    "    features = df.columns.tolist()\n",
    "    # features for all players\n",
    "    shared = features[:5]\n",
    "    # features for specific player\n",
    "    a1 = features[6:9]\n",
    "    a2 = features[9:12]\n",
    "    a3 = features[12:15]\n",
    "    a4 = features[15:18]\n",
    "    b1 = features[18:21]\n",
    "    b2 = features[21:24]\n",
    "    b3 = features[24:27]\n",
    "    b4 = features[27:]\n",
    "    # group players by team\n",
    "    if drop:\n",
    "        a_team = [a2, a3, a4]\n",
    "    else:\n",
    "        a_team = [a1, a2, a3, a4]\n",
    "    b_team = [b1, b2, b3, b4]\n",
    "\n",
    "    # initialize temporaty storage list\n",
    "    tmp = collections.deque()\n",
    "    # iterate through rows\n",
    "    for index, row in df.iterrows():\n",
    "        # get features common to both teams\n",
    "        both = [index] + row[shared].tolist()\n",
    "        # append features for each a team player\n",
    "        for player in a_team:\n",
    "            tmp.append(both + row[player].tolist() + [row.win == 'alpha'])\n",
    "        # append features for each b team player\n",
    "        for player in b_team:\n",
    "            tmp.append(both + row[player].tolist() + [row.win == 'bravo'])\n",
    "\n",
    "    #create new column names, including the index of the match the data was from\n",
    "    new_cols = ['match'] + shared + ['weapon', 'rank', 'level', 'win']\n",
    "    # return new koalas database\n",
    "    return ks.DataFrame(list(tmp), columns=new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = splatoon_explode(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a spark parquet file of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_spark_io('data/explode.parquet', format='parquet')\n",
    "# check file\n",
    "ks.read_spark_io('data/explode.parquet', format='parquet').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
